from fastapi import FastAPI, UploadFile, File, HTTPException, WebSocket, WebSocketDisconnect, Body
from fastapi.middleware.cors import CORSMiddleware
from db import get_hypergraph, getFrequentVertices, get_vertices, get_hyperedges, get_vertice, get_vertice_neighbor, get_hyperedge_neighbor_server, add_vertex, add_hyperedge, delete_vertex, delete_hyperedge, update_vertex, update_hyperedge, get_hyperedge_detail, db_manager
from file_manager import file_manager
from translations import t
import json
import os
import asyncio
import numpy as np
import logging
import sys
import importlib.util
from pathlib import Path
from pydantic import BaseModel, Field
from typing import List
from io import StringIO

# æ·»åŠ  HyperRAG ç›¸å…³å¯¼å…¥
# è‹¥å°šä¸å¯å¯¼å…¥ï¼Œåˆ™å‘ä¸Šé€çº§æŸ¥æ‰¾å«æœ‰ hyperrag åŒ…çš„ç›®å½•ï¼Œå¹¶æŠŠâ€œå…¶çˆ¶ç›®å½•â€åŠ åˆ° sys.path
if importlib.util.find_spec("hyperrag") is None:
    for parent in Path(__file__).resolve().parents:
        if (parent / "hyperrag" / "__init__.py").exists():
            sys.path.insert(0, str(parent))  # æ³¨æ„æ˜¯çˆ¶ç›®å½•ï¼Œä¸æ˜¯ â€¦/hyperrag
            break

try:
    from hyperrag import HyperRAG, QueryParam
    from hyperrag.utils import EmbeddingFunc
    from hyperrag.llm import openai_embedding, openai_complete_if_cache
    HYPERRAG_AVAILABLE = True
except ImportError as e:
    print(f"HyperRAG not available: {e}")
    HYPERRAG_AVAILABLE = False


# è®¾ç½®æ–‡ä»¶è·¯å¾„
SETTINGS_FILE = "settings.json"

# Configure main logger
main_logger = logging.getLogger("main")
main_logger.setLevel(logging.DEBUG)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "Hyper-RAG"}

@app.get("/test-logging")
async def test_logging():
    """Test endpoint to verify logging output"""
    import logging
    test_logger = logging.getLogger("test")
    
    # Test with a very long string
    long_string = "A" * 3000
    test_logger.info(f"Test long string (3000 chars): {long_string}")
    
    # Also test the hyper_rag logger
    hyper_logger = logging.getLogger("hyper_rag")
    hyper_logger.info(f"Hyper RAG long string test: {long_string}")
    
    # Test main_logger
    main_logger.info(f"Main logger long string test: {long_string}")
    
    return {"message": "Check console for logging output", "string_length": len(long_string)}


@app.get("/db")
async def db(database: str = None):
    """
    Get complete hypergraph data in JSON format
    èŽ·å–å…¨éƒ¨æ•°æ®json
    """
    try:
        data = get_hypergraph(database)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/vertices")
async def get_vertices_function(database: str = None, page: int = None, page_size: int = None):
    """
    Get list of vertices
    èŽ·å–verticesåˆ—è¡¨
    """
    try:
        data = getFrequentVertices(database, page, page_size)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/hyperedges")
async def get_hypergraph_function(database: str = None, page: int = None, page_size: int = None):
    """
    Get list of hyperedges
    èŽ·å–hyperedgesåˆ—è¡¨
    """
    try:
        data = get_hyperedges(database, page, page_size)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/hyperedges/{hyperedge_id}")
async def get_hyperedge(hyperedge_id: str, database: str = None):
    """
    Get details of a specific hyperedge
    èŽ·å–æŒ‡å®šhyperedgeçš„è¯¦æƒ…
    """
    try:
        hyperedge_id = hyperedge_id.replace("%20", " ")
        vertices = hyperedge_id.split("|*|")
        data = get_hyperedge_detail(vertices, database)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/vertices/{vertex_id}")
async def get_vertex(vertex_id: str, database: str = None):
    """
    Get JSON data for a specific vertex
    èŽ·å–æŒ‡å®švertexçš„json
    """
    vertex_id = vertex_id.replace("%20", " ")
    try:
        data = get_vertice(vertex_id, database)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/vertices_neighbor/{vertex_id}")
async def get_vertex_neighbor(vertex_id: str, database: str = None):
    """
    Get neighbors of a specific vertex
    èŽ·å–æŒ‡å®švertexçš„neighbor
    """
    vertex_id = vertex_id.replace("%20", " ")
    try:
        data = get_vertice_neighbor(vertex_id, database)
        return data
    except Exception as e:
        return {"error": str(e)}

@app.get("/db/hyperedge_neighbor/{hyperedge_id}")
async def get_hyperedge_neighbor(hyperedge_id: str, database: str = None):
    """
    Get neighbors of a specific hyperedge
    èŽ·å–æŒ‡å®šhyperedgeçš„neighbor
    """
    hyperedge_id = hyperedge_id.replace("%20", " ")
    hyperedge_id = hyperedge_id.replace("*", "#")
    print(hyperedge_id)
    try:
        data = get_hyperedge_neighbor_server(hyperedge_id, database)
        return data
    except Exception as e:
        return {"error": str(e)}

class VertexModel(BaseModel):
    vertex_id: str
    entity_name: str = ""
    entity_type: str = ""
    description: str = ""
    additional_properties: str = ""
    database: str = None

class HyperedgeModel(BaseModel):
    vertices: list
    keywords: str = ""
    summary: str = ""
    database: str = None

class VertexUpdateModel(BaseModel):
    entity_name: str = ""
    entity_type: str = ""
    description: str = ""
    additional_properties: str = ""
    database: str = None

class HyperedgeUpdateModel(BaseModel):
    keywords: str = ""
    summary: str = ""
    database: str = None

@app.post("/db/vertices")
async def create_vertex(vertex: VertexModel):
    """
    Create a new vertex
    åˆ›å»ºæ–°çš„vertex
    """
    try:
        result = add_vertex(vertex.vertex_id, {
            "entity_name": vertex.entity_name,
            "entity_type": vertex.entity_type,
            "description": vertex.description,
            "additional_properties": vertex.additional_properties
        }, vertex.database)
        return {"success": True, "message": "Vertex created successfully", "data": result}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.post("/db/hyperedges")
async def create_hyperedge(hyperedge: HyperedgeModel):
    """
    Create a new hyperedge
    åˆ›å»ºæ–°çš„hyperedge
    """
    try:
        result = add_hyperedge(hyperedge.vertices, {
            "keywords": hyperedge.keywords,
            "summary": hyperedge.summary
        }, hyperedge.database)
        return {"success": True, "message": "Hyperedge created successfully", "data": result}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.put("/db/vertices/{vertex_id}")
async def update_vertex_endpoint(vertex_id: str, vertex: VertexUpdateModel):
    """
    Update vertex information
    æ›´æ–°vertexä¿¡æ¯
    """
    try:
        vertex_id = vertex_id.replace("%20", " ")
        result = update_vertex(vertex_id, {
            "entity_name": vertex.entity_name,
            "entity_type": vertex.entity_type,
            "description": vertex.description,
            "additional_properties": vertex.additional_properties
        }, vertex.database)
        return {"success": True, "message": "Vertex updated successfully", "data": result}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.put("/db/hyperedges/{hyperedge_id}")
async def update_hyperedge_endpoint(hyperedge_id: str, hyperedge: HyperedgeUpdateModel):
    """
    Update hyperedge information
    æ›´æ–°hyperedgeä¿¡æ¯
    """
    try:
        hyperedge_id = hyperedge_id.replace("%20", " ")
        vertices = hyperedge_id.split("|*|")
        result = update_hyperedge(vertices, {
            "keywords": hyperedge.keywords,
            "summary": hyperedge.summary
        }, hyperedge.database)
        return {"success": True, "message": "Hyperedge updated successfully", "data": result}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.delete("/db/vertices/{vertex_id}")
async def delete_vertex_endpoint(vertex_id: str, database: str = None):
    """
    Delete a vertex
    åˆ é™¤vertex
    """
    try:
        vertex_id = vertex_id.replace("%20", " ")
        result = delete_vertex(vertex_id, database)
        return {"success": True, "message": "Vertex deleted successfully"}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.delete("/db/hyperedges/{hyperedge_id}")
async def delete_hyperedge_endpoint(hyperedge_id: str, database: str = None):
    """
    Delete a hyperedge
    åˆ é™¤hyperedge
    """
    try:
        hyperedge_id = hyperedge_id.replace("%20", " ")
        vertices = hyperedge_id.split("|*|")
        result = delete_hyperedge(vertices, database)
        return {"success": True, "message": "Hyperedge deleted successfully"}
    except Exception as e:
        return {"success": False, "message": str(e)}

# è®¾ç½®ç›¸å…³çš„APIæŽ¥å£

class SettingsModel(BaseModel):
    apiKey: str = ""
    modelProvider: str = "openai"
    modelName: str = "gpt-5-mini"
    baseUrl: str = "https://api.openai.com/v1"
    selectedDatabase: str = ""
    maxTokens: int = 2000
    temperature: float = 0.7
    # HyperRAG embedding model settings
    embeddingModel: str = "text-embedding-3-small"
    embeddingDim: int = 1536
    # Language setting for backend
    language: str = "en-US"  # en-US or zh-CN

class APITestModel(BaseModel):
    apiKey: str
    baseUrl: str
    modelName: str
    modelProvider: str

class DatabaseTestModel(BaseModel):
    database: str

@app.get("/settings")
async def get_settings():
    """
    Get system settings
    èŽ·å–ç³»ç»Ÿè®¾ç½®
    """
    try:
        if os.path.exists(SETTINGS_FILE):
            with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
                settings = json.load(f)
            # ä¸è¿”å›žæ•æ„Ÿä¿¡æ¯å¦‚API Key
            settings_safe = settings.copy()
            if 'apiKey' in settings_safe:
                settings_safe['apiKey'] = '***' if settings_safe['apiKey'] else ''
            return settings_safe
        else:
            # è¿”å›žé»˜è®¤è®¾ç½®
            return {
                "apiKey": "",
                "modelProvider": "openai",
                "modelName": "gpt-4o-mini",
                "baseUrl": "https://api.openai.com/v1",
                "selectedDatabase": "",
                "maxTokens": 2000,
                "temperature": 0.7,
                "embeddingModel": "text-embedding-3-small",
                "embeddingDim": 1536,
                "language": "en-US"
            }
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.post("/settings")
async def save_settings(settings: SettingsModel):
    """
    Save system settings
    ä¿å­˜ç³»ç»Ÿè®¾ç½®
    """
    try:
        settings_dict = settings.dict()
        
        # å¦‚æžœapiKeyæ˜¯***ï¼Œåˆ™ä¿æŒåŽŸæœ‰çš„apiKeyä¸å˜
        if settings_dict.get('apiKey') == '***':
            # è¯»å–çŽ°æœ‰è®¾ç½®ä¸­çš„apiKey
            if os.path.exists(SETTINGS_FILE):
                with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
                    existing_settings = json.load(f)
                # ä¿æŒåŽŸæœ‰çš„apiKey
                settings_dict['apiKey'] = existing_settings.get('apiKey', '')
            else:
                # å¦‚æžœæ²¡æœ‰çŽ°æœ‰è®¾ç½®æ–‡ä»¶ï¼Œåˆ™è®¾ä¸ºç©ºå­—ç¬¦ä¸²
                settings_dict['apiKey'] = ''
        
        with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(settings_dict, f, ensure_ascii=False, indent=2)
        return {"success": True, "message": t('settings_save_success')}
    except Exception as e:
        return {"success": False, "message": str(e)}

@app.get("/databases")
async def get_databases(lang: str = "en-US"):
    """
    Get list of available databases
    èŽ·å–å¯ç”¨æ•°æ®åº“åˆ—è¡¨
    """
    try:
        databases = []
        
        # ä½¿ç”¨db_managerèŽ·å–æ•°æ®åº“åˆ—è¡¨
        database_files = db_manager.list_databases()
        
        for file in database_files:
            # Generate description based on language setting from translations
            base_name = file.replace('.hgdb', '')
            description = f"{base_name} {t('hypergraph_suffix')}"
            
            databases.append({
                "name": file,
                "description": description
            })
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶ï¼Œè¿”å›žé»˜è®¤åˆ—è¡¨
        if not databases:
            databases = []
        
        return databases
    except Exception as e:
        return {"success": False, "message": str(e), "data": []}

class CreateDatabaseRequest(BaseModel):
    """Request model for creating a new database"""
    name: str = Field(..., description="Name of the database to create (without .hgdb extension)", example="my_knowledge_base")

@app.post("/create-database")
async def create_database(request: CreateDatabaseRequest):
    """
    Create a new empty database
    åˆ›å»ºæ–°çš„ç©ºæ•°æ®åº“
    """
    try:
        database_name = request.name
        if not database_name:
            return {"success": False, "message": "Database name is required"}
        
        # Ensure it doesn't have .hgdb extension (we'll add it)
        if database_name.endswith('.hgdb'):
            database_name = database_name[:-5]
        
        # Check if database already exists
        existing_databases = db_manager.list_databases()
        if database_name in existing_databases:
            return {"success": False, "message": f"Database '{database_name}' already exists"}
        
        # Create directory structure
        database_dir = os.path.join(db_manager.cache_dir, database_name)
        Path(database_dir).mkdir(parents=True, exist_ok=True)
        
        # Create empty .hgdb file path
        database_path = os.path.join(database_dir, "hypergraph_chunk_entity_relation.hgdb")
        
        # Initialize empty HypergraphDB (it will create the file when saved)
        from hyperdb import HypergraphDB
        new_db = HypergraphDB(storage_file=database_path)
        
        # Save empty database to create the file
        new_db.save(Path(database_path))
        
        # Get current embedding dimensions from settings
        settings = {}
        if os.path.exists(SETTINGS_FILE):
            with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
                settings = json.load(f)
        
        # Determine embedding dimensions based on provider and model
        embedding_provider = settings.get("embeddingProvider", settings.get("modelProvider", "openai"))
        embedding_model = settings.get("embeddingModel", "text-embedding-3-small")
        
        if embedding_provider == "local":
            # Local models - common dimensions
            if "nomic" in embedding_model.lower():
                embedding_dim = 768
            elif "bge" in embedding_model.lower():
                embedding_dim = 1024
            elif "e5" in embedding_model.lower():
                embedding_dim = 1024
            else:
                embedding_dim = settings.get("embeddingDim", 768)  # Default to 768 or user-specified
        elif embedding_provider == "gemini":
            embedding_dim = settings.get("embeddingDim", 768)
        else:
            # OpenAI models
            if "text-embedding-3-large" in embedding_model:
                embedding_dim = 3072
            elif "text-embedding-3-small" in embedding_model:
                embedding_dim = 1536
            elif "text-embedding-ada-002" in embedding_model:
                embedding_dim = 1536
            else:
                embedding_dim = settings.get("embeddingDim", 1536)
        
        # Initialize vector database files with correct dimensions
        from nano_vectordb import NanoVectorDB
        
        # Create empty vector databases for chunks, entities, and relationships
        for vdb_name in ["vdb_chunks", "vdb_entities", "vdb_relationships"]:
            vdb_path = os.path.join(database_dir, f"{vdb_name}.json")
            vdb = NanoVectorDB(embedding_dim, storage_file=vdb_path)
            vdb.save()
        
        # Create empty KV store files
        for kv_name in ["kv_store_full_docs", "kv_store_text_chunks", "kv_store_llm_response_cache"]:
            kv_path = os.path.join(database_dir, f"{kv_name}.json")
            with open(kv_path, 'w') as f:
                json.dump({}, f)
        
        main_logger.info(f"Created new database: {database_name} with embedding_dim: {embedding_dim}")
        
        return {
            "success": True, 
            "message": f"Database '{database_name}' created successfully with {embedding_dim}-dimensional embeddings",
            "database": database_name
        }
    except Exception as e:
        main_logger.error(f"Failed to create database: {str(e)}")
        return {"success": False, "message": str(e)}

@app.post("/test-api")
async def test_api_connection(api_test: APITestModel):
    """
    Test API connection
    æµ‹è¯•APIè¿žæŽ¥
    """
    try:
        from openai import OpenAI
        
        # æ ¹æ®ä¸åŒçš„æ¨¡åž‹æä¾›å•†è¿›è¡Œæµ‹è¯•
        if api_test.modelProvider == "gemini":
            # Test Gemini API
            from google import genai
            client = genai.Client(api_key=api_test.apiKey)
            # Simple test to check if API key is valid
            response = client.models.generate_content(
                model=api_test.modelName or "gemini-2.5-flash-lite",
                contents="Hello, this is a test. Reply with 'API connection successful'."
            )
            return {"success": True, "message": "Gemini API connection successful"}
        elif api_test.modelProvider == "openai":
            client = OpenAI(
                api_key=api_test.apiKey,
                base_url=api_test.baseUrl
            )
            
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = client.chat.completions.create(
                model=api_test.modelName,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            return {"success": True, "message": t('api_test_success')}
            
        elif api_test.modelProvider == "anthropic":
            # å¯¹äºŽAnthropicï¼Œå¯ä»¥æ·»åŠ ç›¸åº”çš„æµ‹è¯•é€»è¾‘
            return {"success": True, "message": t('anthropic_api_test_success')}
            
        else:
            # å¯¹äºŽå…¶ä»–æä¾›å•†ï¼Œè¿›è¡Œé€šç”¨æµ‹è¯•
            return {"success": True, "message": t('api_test_success')}
            
    except Exception as e:
        return {"success": False, "message": t('api_test_failed', error=str(e))}

@app.post("/test-database")
async def test_database_connection(db_test: DatabaseTestModel):
    """
    Test database connection
    æµ‹è¯•æ•°æ®åº“è¿žæŽ¥
    """
    try:
        # ä½¿ç”¨db_manageræµ‹è¯•æ•°æ®åº“è¿žæŽ¥
        db = db_manager.get_database(db_test.database)
        
        # å°è¯•èŽ·å–æ•°æ®åº“çš„åŸºæœ¬ä¿¡æ¯æ¥éªŒè¯è¿žæŽ¥
        vertices_count = len(db.all_v)
        edges_count = len(db.all_e)
        
        return {
            "success": True, 
            "message": t('database_test_success'),
            "info": {
                "vertices_count": vertices_count,
                "edges_count": edges_count,
                "database": db_test.database
            }
        }
        
    except Exception as e:
        return {"success": False, "message": t('database_test_failed', error=str(e))}


# å…¨å±€ HyperRAG å®žä¾‹ - æ”¹ä¸ºå­—å…¸æ¥æ”¯æŒå¤šæ•°æ®åº“
hyperrag_instances = {}
hyperrag_working_dir = "hyperrag_cache"

async def get_hyperrag_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs) -> str:
    """
    HyperRAG-specific LLM function, using async version
    HyperRAG ä¸“ç”¨çš„ LLM å‡½æ•°ï¼Œä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬
    """
    # Initialize variables that might be used in error handler
    estimated_tokens = 0
    model_name = "unknown"
    model_provider = "unknown"
    base_url = None
    
    try:
        # Load settings first to get model name for token counting
        with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        
        model_name = settings.get("modelName", "gpt-4o-mini")
        api_key = settings.get("apiKey")
        # If api_key is a list (for rotation), use the first one for LLM calls
        if isinstance(api_key, list) and api_key:
            llm_api_key = api_key[0]
        else:
            llm_api_key = api_key
        base_url = settings.get("baseUrl")
        model_provider = settings.get("modelProvider", "openai")
        
        # Enhanced logging for debugging token length issues
        main_logger.info(f"=== LLM Request Debug Info ===")
        main_logger.info(f"Model: {model_name} (Provider: {model_provider})")
        main_logger.info(f"Prompt length (chars): {len(prompt)}")
        main_logger.info(f"Prompt length (words approx): {len(prompt.split())}")
        main_logger.info(f"Prompt preview (first 200 chars): {prompt[:200]}...")
        
        if system_prompt:
            main_logger.info(f"System prompt length (chars): {len(system_prompt)}")
            main_logger.info(f"System prompt length (words approx): {len(system_prompt.split())}")
            main_logger.info(f"System prompt preview (first 200 chars): {system_prompt[:200]}...")
        
        if history_messages:
            total_history_length = sum(len(msg.get("content", "")) for msg in history_messages)
            main_logger.info(f"History messages count: {len(history_messages)}")
            main_logger.info(f"Total history content length (chars): {total_history_length}")
        
        # Use tiktoken for accurate token counting
        import tiktoken
        
        # Get the appropriate encoder for the model
        try:
            # Try to get encoder for the specific model
            if "gpt" in model_name.lower():
                encoder = tiktoken.encoding_for_model(model_name)
            else:
                # Use cl100k_base as default (GPT-4 encoder)
                encoder = tiktoken.get_encoding("cl100k_base")
        except:
            # Fallback to cl100k_base
            encoder = tiktoken.get_encoding("cl100k_base")
        
        # Count tokens accurately
        prompt_tokens = len(encoder.encode(prompt))
        system_tokens = len(encoder.encode(system_prompt)) if system_prompt else 0
        history_tokens = 0
        if history_messages:
            for msg in history_messages:
                content = msg.get("content", "")
                history_tokens += len(encoder.encode(content))
        
        estimated_tokens = prompt_tokens + system_tokens + history_tokens
        
        main_logger.info(f"=== Accurate Token Count (using tiktoken) ===")
        main_logger.info(f"Prompt tokens: {prompt_tokens}")
        main_logger.info(f"System prompt tokens: {system_tokens}")
        main_logger.info(f"History tokens: {history_tokens}")
        main_logger.info(f"Total tokens: {estimated_tokens}")
        
        # Warn if approaching common context limits
        if estimated_tokens > 120000:
            main_logger.warning(f"Token count ({estimated_tokens}) exceeds 128k context window!")
        elif estimated_tokens > 30000:
            main_logger.warning(f"Token count ({estimated_tokens}) exceeds 32k context window!")
        elif estimated_tokens > 14000:
            main_logger.warning(f"Token count ({estimated_tokens}) exceeds 16k context window!")
        elif estimated_tokens > 7000:
            main_logger.warning(f"Token count ({estimated_tokens}) exceeds 8k context window!")
        elif estimated_tokens > 3500:
            main_logger.warning(f"Token count ({estimated_tokens}) exceeds 4k context window!")
        
        # Log kwargs for additional parameters
        if kwargs:
            main_logger.info(f"Additional kwargs: {list(kwargs.keys())}")
            if 'max_tokens' in kwargs:
                main_logger.info(f"max_tokens parameter: {kwargs['max_tokens']}")
            if 'temperature' in kwargs:
                main_logger.info(f"temperature parameter: {kwargs['temperature']}")
        
        main_logger.info(f"=== End Debug Info ===")
        
        main_logger.info(t('llm_call_start', length=len(prompt)))
        if system_prompt:
            main_logger.info(t('system_prompt_length', length=len(system_prompt)))
        
        main_logger.info(t('using_model', model=model_name, url=base_url))
        
        if model_provider == "gemini":
            # Use Gemini for text generation
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=llm_api_key)
            
            # Build conversation history using proper Content types
            contents = []
            
            # Add system prompt as initial user/model exchange if provided
            if system_prompt:
                contents.append(types.Content(
                    role='user',
                    parts=[types.Part.from_text(text=system_prompt)]
                ))
                contents.append(types.Content(
                    role='model', 
                    parts=[types.Part.from_text(text="Understood. I'll follow these instructions.")]
                ))
            
            # Add history messages
            for msg in history_messages:
                role = 'user' if msg.get("role") == "user" else 'model'
                contents.append(types.Content(
                    role=role,
                    parts=[types.Part.from_text(text=msg.get("content", ""))]
                ))
            
            # Add current prompt
            contents.append(types.Content(
                role='user',
                parts=[types.Part.from_text(text=prompt)]
            ))
            
            # Retry logic for 503 errors
            max_retries = 100
            retry_delay = 30  # seconds (increased by factor of 6)
            
            for attempt in range(max_retries):
                try:
                    response_obj = client.models.generate_content(
                        model=model_name or "gemini-2.5-flash-lite",
                        contents=contents
                    )
                    response = response_obj.text
                    break  # Success, exit retry loop
                    
                except Exception as e:
                    error_str = str(e)
                    if "503" in error_str or "overloaded" in error_str.lower() or "UNAVAILABLE" in error_str:
                        if attempt < max_retries - 1:
                            main_logger.info(f"Model overloaded (503), retrying in {retry_delay}s (attempt {attempt + 2}/{max_retries})")
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # Exponential backoff
                            continue
                        else:
                            main_logger.error(f"Max retries reached for 503 error")
                            raise
                    else:
                        # Not a 503 error, raise immediately
                        raise
        else:
            # Use OpenAI-compatible API
            main_logger.info(f"Making OpenAI-compatible API call:")
            main_logger.info(f"  - Model: {model_name}")
            main_logger.info(f"  - Base URL: {base_url}")
            main_logger.info(f"  - Has API key: {bool(llm_api_key)}")
            main_logger.info(f"  - Additional kwargs keys: {list(kwargs.keys()) if kwargs else 'None'}")
            
            response = await openai_complete_if_cache(
                model_name,
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=llm_api_key,
                base_url=base_url,
                **kwargs,
            )
        
        main_logger.info(t('llm_call_complete', length=len(response)))
        return response
        
    except Exception as e:
        error_str = str(e)
        main_logger.error(f"=== LLM Call Failed ===")
        main_logger.error(f"Error type: {type(e).__name__}")
        main_logger.error(f"Error message: {error_str}")
        
        # Check for specific error patterns
        if "400" in error_str or "tokens to keep" in error_str:
            main_logger.error("This appears to be a token context length issue!")
            main_logger.error(f"Model being used: {model_name}")
            main_logger.error(f"Provider: {model_provider}")
            main_logger.error(f"Base URL: {base_url}")
            
            # Log the actual request parameters that caused the issue
            main_logger.error(f"Failed request details:")
            main_logger.error(f"  - Prompt length (chars): {len(prompt)}")
            if system_prompt:
                main_logger.error(f"  - System prompt length (chars): {len(system_prompt)}")
            if history_messages:
                main_logger.error(f"  - History messages: {len(history_messages)} messages")
            main_logger.error(f"  - Actual total tokens (tiktoken): {estimated_tokens}")
            
            # Provide suggestions
            main_logger.error("Suggestions:")
            main_logger.error("  1. Check if the model's context window is large enough")
            main_logger.error("  2. Reduce the prompt or system prompt length")
            main_logger.error("  3. Clear history messages if they're too long")
            main_logger.error("  4. Switch to a model with a larger context window")
        
        main_logger.error(f"=== End Error Details ===")
        main_logger.error(t('llm_call_failed', error=str(e)))
        raise

async def get_hyperrag_embedding_func(texts: list[str]) -> np.ndarray:
    """
    HyperRAG-specific embedding function
    HyperRAG ä¸“ç”¨çš„åµŒå…¥å‡½æ•°
    """
    try:
        # Show what type of content is being embedded
        if texts and len(texts) > 0:
            sample_text = texts[0][:100] + "..." if len(texts[0]) > 100 else texts[0]
            main_logger.info(f"[Embedding] Processing {len(texts)} texts - Sample: {sample_text}")
        
        import time
        start_time = time.time()
        
        main_logger.info(t('text_embedding_start', count=len(texts)))
        main_logger.info(t('text_total_length', length=sum(len(text) for text in texts)))
        
        # ä»Žè®¾ç½®æ–‡ä»¶è¯»å–é…ç½®
        with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        
        embedding_model = settings.get("embeddingModel", "text-embedding-3-small")
        api_key = settings.get("apiKey")
        base_url = settings.get("baseUrl")
        model_provider = settings.get("modelProvider", "openai")
        embedding_dim = settings.get("embeddingDim", 768)
        
        main_logger.info(t('using_embedding_model', model=embedding_model))
        
        # Check if we should use a separate embedding provider
        embedding_provider = settings.get("embeddingProvider", model_provider)
        embedding_api_key = settings.get("embeddingApiKey", api_key)
        embedding_base_url = settings.get("embeddingBaseUrl", base_url)
        
        # Check embedding provider
        if embedding_provider == "local":
            # Use local embedding server
            from hyperrag.llm import local_embedding
            embeddings = await local_embedding(
                texts,
                base_url=embedding_base_url or "http://localhost:1234",
                model=embedding_model,
                api_key=embedding_api_key,
                batch_size=settings.get("embeddingBatchSize", 32)
            )
        elif embedding_provider == "gemini":
            # Use Gemini embeddings
            from hyperrag.llm import gemini_embedding
            embeddings = await gemini_embedding(
                texts,
                model=embedding_model,
                api_key=embedding_api_key,
                embedding_dim=embedding_dim
            )
        else:
            # Use OpenAI embeddings (for openai, azure, custom, etc.)
            # If api_key is a list, use the first one (OpenAI doesn't support rotation yet)
            if isinstance(embedding_api_key, list) and embedding_api_key:
                embedding_api_key = embedding_api_key[0]
            embeddings = await openai_embedding(
                texts,
                model=embedding_model,
                api_key=embedding_api_key,
                base_url=embedding_base_url,
            )
        
        elapsed_time = time.time() - start_time
        main_logger.info(f"[Embedding] Completed in {elapsed_time:.2f} seconds")
        main_logger.info(t('text_embedding_complete', dimensions=embeddings.shape))
        return embeddings
        
    except Exception as e:
        main_logger.error(t('text_embedding_failed', error=str(e)))
        raise

def get_or_create_hyperrag(database: str = None):
    """
    Get or create HyperRAG instance for specified database
    èŽ·å–æˆ–åˆ›å»ºæŒ‡å®šæ•°æ®åº“çš„ HyperRAG å®žä¾‹
    """
    global hyperrag_instances
    
    if not HYPERRAG_AVAILABLE:
        main_logger.error(t('hyperrag_unavailable'))
        raise RuntimeError("HyperRAG is not available")
    
    # å¦‚æžœæ²¡æœ‰æŒ‡å®šæ•°æ®åº“ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®åº“
    if database is None:
        database = db_manager.default_database
        main_logger.info(t('using_default_database', database=database))
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ•°æ®åº“çš„å®žä¾‹
    if database not in hyperrag_instances:
        main_logger.info(t('create_new_hyperrag_instance', database=database))
        
        # ä½¿ç”¨æ•°æ®åº“åä½œä¸ºå·¥ä½œç›®å½•ï¼ˆåŽ»æŽ‰.hgdbåŽç¼€ï¼‰
        if database.endswith('.hgdb'):
            db_dir_name = database.replace('.hgdb', '')
        else:
            db_dir_name = database
            
        # HyperRAG å·¥ä½œç›®å½•ç›´æŽ¥ä½¿ç”¨ hyperrag_cache ä¸‹çš„æ•°æ®åº“æ–‡ä»¶å¤¹
        db_working_dir = os.path.join(hyperrag_working_dir, db_dir_name)
        Path(db_working_dir).mkdir(parents=True, exist_ok=True)
        
        main_logger.info(t('hyperrag_working_dir', dir=db_working_dir))
        
        # Get embedding dimensions from settings
        with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        
        embedding_provider = settings.get("embeddingProvider", settings.get("modelProvider", "openai"))
        embedding_model = settings.get("embeddingModel", "text-embedding-3-small")
        
        # Determine embedding dimensions based on provider and model
        if embedding_provider == "local":
            # Local models - common dimensions
            if "nomic" in embedding_model.lower():
                embedding_dim = 768
            elif "bge" in embedding_model.lower():
                embedding_dim = 1024
            elif "e5" in embedding_model.lower():
                embedding_dim = 1024
            else:
                embedding_dim = settings.get("embeddingDim", 768)  # Default to 768 or user-specified
        elif embedding_provider == "gemini":
            embedding_dim = settings.get("embeddingDim", 768)
        else:
            # OpenAI models
            if "text-embedding-3-large" in embedding_model:
                embedding_dim = 3072
            elif "text-embedding-3-small" in embedding_model:
                embedding_dim = 1536
            elif "text-embedding-ada-002" in embedding_model:
                embedding_dim = 1536
            else:
                embedding_dim = settings.get("embeddingDim", 1536)
        
        # Check if we should use smart LLM wrapper with structured output support
        if settings.get("useStructuredOutput", False):
            # Use smart wrapper for automatic structured output handling
            from hyperrag.llm_structured_wrapper import create_smart_llm_func
            llm_func = create_smart_llm_func(settings)
            main_logger.info("Using smart LLM wrapper with structured output support")
        else:
            # Use traditional LLM function
            llm_func = get_hyperrag_llm_func
            main_logger.info("Using traditional LLM function")
        
        # åˆå§‹åŒ– HyperRAG å®žä¾‹
        hyperrag_instances[database] = HyperRAG(
            working_dir=db_working_dir,
            llm_model_func=llm_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=embedding_dim,
                max_token_size=8192,
                func=get_hyperrag_embedding_func
            ),
            entity_extract_max_gleaning=settings.get("entityExtractMaxGleaning", 1),
            chunk_token_size=settings.get("chunkTokenSize", 1200),
            chunk_overlap_token_size=settings.get("chunkOverlapTokenSize", 100),
        )
        
        main_logger.info(t('hyperrag_instance_created', database=database))
    else:
        main_logger.info(t('use_existing_hyperrag_instance', database=database))
    
    return hyperrag_instances[database]


class Message(BaseModel):
    message: str

@app.post("/process_message")
async def process_message(msg: Message):
    user_message = msg.message
    try:
        response_message = await get_hyperrag_llm_func(prompt=user_message)
    except Exception as e:
        return {"response": str(e)} 
    return {"response": response_message}

# HyperRAG é—®ç­”ç›¸å…³æŽ¥å£

class DocumentModel(BaseModel):
    content: str
    retries: int = 3
    database: str = None  # æ·»åŠ æ•°æ®åº“å‚æ•°

class QueryModel(BaseModel):
    question: str
    mode: str = "hyper"  # hyper, hyper-lite, naive
    top_k: int = 60
    max_token_for_text_unit: int = 1600
    max_token_for_entity_context: int = 300 
    max_token_for_relation_context: int = 1600 
    only_need_context: bool = False
    response_type: str = "Multiple Paragraphs"
    database: str = None  # æ·»åŠ æ•°æ®åº“å‚æ•°

@app.post("/hyperrag/insert")
async def insert_document(doc: DocumentModel):
    """
    Insert document into specified database's HyperRAG
    å‘æŒ‡å®šæ•°æ®åº“çš„ HyperRAG æ’å…¥æ–‡æ¡£
    """
    if not HYPERRAG_AVAILABLE:
        return {"success": False, "message": "HyperRAG is not available"}
    
    try:
        rag = get_or_create_hyperrag(doc.database)
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(doc.retries):
            try:
                await rag.ainsert(doc.content)
                return {
                    "success": True, 
                    "message": "Document inserted successfully",
                    "database": doc.database or "default"
                }
            except Exception as e:
                if attempt == doc.retries - 1:
                    raise e
                print(f"Insert attempt {attempt + 1} failed: {e}. Retrying...")
                await asyncio.sleep(2)
                
    except Exception as e:
        return {"success": False, "message": f"Failed to insert document: {str(e)}"}

@app.post("/hyperrag/query")
async def query_hyperrag(query: QueryModel):
    """
    Query HyperRAG using specified database for Q&A
    ä½¿ç”¨æŒ‡å®šæ•°æ®åº“çš„ HyperRAG è¿›è¡Œé—®ç­”æŸ¥è¯¢
    """
    main_logger.info("=" * 80)
    main_logger.info("ðŸ” HyperRAG Query Endpoint Called")
    main_logger.info(f"ðŸ“ Question: {query.question}")
    main_logger.info(f"ðŸ“Š Database: {query.database}")
    main_logger.info(f"âš™ï¸ Mode: {query.mode}")
    main_logger.info("=" * 80)
    
    if not HYPERRAG_AVAILABLE:
        main_logger.error("âŒ HyperRAG is not available")
        return {"success": False, "message": "HyperRAG is not available"}
    
    try:
        main_logger.info("ðŸ”„ Getting or creating HyperRAG instance...")
        rag = get_or_create_hyperrag(query.database)
        main_logger.info(f"âœ… HyperRAG instance obtained for database: {query.database}")
        
        # åˆ›å»ºæŸ¥è¯¢å‚æ•°
        main_logger.info("ðŸ“‹ Creating query parameters...")
        param = QueryParam(
            mode=query.mode,
            top_k=query.top_k,
            max_token_for_text_unit=query.max_token_for_text_unit,
            max_token_for_entity_context=query.max_token_for_entity_context,
            max_token_for_relation_context=query.max_token_for_relation_context,
            only_need_context=query.only_need_context,
            response_type=query.response_type,
            return_type='json'
        )
        main_logger.info(f"ðŸ“‹ Query params: mode={param.mode}, top_k={param.top_k}, response_type={param.response_type}")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        main_logger.info("ðŸš€ Executing HyperRAG query...")
        main_logger.info(f"   Calling rag.aquery with question: {query.question[:100]}...")
        result = await rag.aquery(query.question, param)
        main_logger.info("âœ… Query completed successfully")
        main_logger.info(f"ðŸ“Š Result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
        
        # å¤„ç†ç»“æžœæ ¼å¼
        response_data = {
            "success": True,
            "response": result.get("response", ""),
            "entities": result.get("entities", []),
            "hyperedges": result.get("hyperedges", []),
            "text_units": result.get("text_units", []),
            "mode": query.mode,
            "question": query.question,
            "database": query.database or "default"
        }
        
        main_logger.info(f"âœ… Returning response with {len(response_data.get('entities', []))} entities")
        return response_data
        
    except Exception as e:
        import traceback
        error_traceback = traceback.format_exc()
        main_logger.error("âŒ Query failed with exception:")
        main_logger.error(f"   Error type: {type(e).__name__}")
        main_logger.error(f"   Error message: {str(e)}")
        main_logger.error(f"   Full traceback:\n{error_traceback}")
        
        # Check if it's a 404 error
        if "404" in str(e):
            main_logger.error("ðŸ”´ 404 Error detected - API endpoint not found")
            main_logger.error("   This usually means the LLM provider doesn't support the requested endpoint")
        
        return {"success": False, "message": f"Query failed: {str(e)}"}

@app.get("/hyperrag/status")
async def get_hyperrag_status(database: str = None):
    """
    Get HyperRAG instance status for specified database
    èŽ·å–æŒ‡å®šæ•°æ®åº“çš„ HyperRAG å®žä¾‹çŠ¶æ€
    """
    try:
        status = {
            "available": HYPERRAG_AVAILABLE,
            "database": database or "default",
            "working_dir": hyperrag_working_dir,
            "instances": list(hyperrag_instances.keys())
        }
        
        if database:
            # èŽ·å–ç‰¹å®šæ•°æ®åº“çš„çŠ¶æ€
            if database in hyperrag_instances:
                instance = hyperrag_instances[database]
                status["initialized"] = True
                try:
                    status["details"] = {
                        "chunk_token_size": instance.chunk_token_size,
                        "llm_model_name": instance.llm_model_name,
                        "embedding_func_available": instance.embedding_func is not None,
                        "working_dir": os.path.join(hyperrag_working_dir, database.replace('.hgdb', ''))
                    }
                except Exception as e:
                    status["details"] = f"Error getting details: {str(e)}"
            else:
                status["initialized"] = False
        else:
            # èŽ·å–æ‰€æœ‰å®žä¾‹çš„æ¦‚è§ˆ
            status["initialized"] = len(hyperrag_instances) > 0
            status["total_instances"] = len(hyperrag_instances)
        
        return status
        
    except Exception as e:
        return {"success": False, "message": f"Failed to get status: {str(e)}"}

@app.delete("/hyperrag/reset")
async def reset_hyperrag(database: str = None):
    """
    Reset HyperRAG instance for specified database, or reset all instances
    é‡ç½®æŒ‡å®šæ•°æ®åº“çš„ HyperRAG å®žä¾‹ï¼Œæˆ–é‡ç½®æ‰€æœ‰å®žä¾‹
    """
    global hyperrag_instances
    
    try:
        if database:
            # é‡ç½®ç‰¹å®šæ•°æ®åº“çš„å®žä¾‹
            if database in hyperrag_instances:
                del hyperrag_instances[database]
                return {
                    "success": True, 
                    "message": f"HyperRAG instance for database '{database}' reset successfully"
                }
            else:
                return {
                    "success": False, 
                    "message": f"No HyperRAG instance found for database '{database}'"
                }
        else:
            # é‡ç½®æ‰€æœ‰å®žä¾‹
            hyperrag_instances = {}
            return {"success": True, "message": "All HyperRAG instances reset successfully"}
            
    except Exception as e:
        return {"success": False, "message": f"Failed to reset: {str(e)}"}

# æ–‡ä»¶ç®¡ç†ç›¸å…³çš„APIæŽ¥å£

class FileEmbedRequest(BaseModel):
    file_ids: List[str]
    chunk_size: int = 1000
    chunk_overlap: int = 200
    database: str = None  # Add database parameter

class BulkDeleteRequest(BaseModel):
    file_ids: List[str]

@app.get("/files")
async def get_files():
    """
    Get list of all uploaded files
    èŽ·å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    """
    try:
        files = file_manager.get_all_files()
        return {"files": files}
    except Exception as e:
        raise HTTPException(status_code=500, detail=t('get_file_list_failed', error=str(e)))

@app.post("/files/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    """
    File upload interface
    ä¸Šä¼ æ–‡ä»¶æŽ¥å£
    """
    print(f"\n{'='*50}")
    print(t('file_upload_start', count=len(files)))
    print(f"{'='*50}")
    
    results = []
    
    for i, file in enumerate(files):
        try:
            print(f"\n{t('upload_file', current=i+1, total=len(files), filename=file.filename)}")
            print(t('file_size', size=file.size if hasattr(file, 'size') else t('unknown')))
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            print(t('reading_file_content'))
            content = await file.read()
            print(f"âœ… {t('file_content_read_complete', size=len(content))}")
            
            # ä¿å­˜æ–‡ä»¶
            print(t('saving_file_locally'))
            file_info = await file_manager.save_uploaded_file(content, file.filename)
            file_info["status"] = "uploaded"
            print(f"âœ… {t('file_save_success', filename=file_info['filename'])}")
            print(f"  - {t('file_id', file_id=file_info['file_id'])}")
            print(f"  - {t('save_path', path=file_info['file_path'])}")
            print(f"  - {t('database', database=file_info['database_name'])}")
            
            results.append(file_info)
            
        except Exception as e:
            error_msg = t('file_upload_failed', filename=file.filename, error=str(e))
            print(f"âŒ {error_msg}")
            main_logger.error(error_msg)
            results.append({
                "filename": file.filename,
                "status": "error",
                "error": str(e)
            })
    
    success_count = len([r for r in results if r.get('status') == 'uploaded'])
    print(f"\n{t('file_upload_complete', success=success_count, total=len(files))}")
    print(f"{'='*50}")
    
    return {"files": results}

@app.delete("/files/{file_id}")
async def delete_file(file_id: str):
    """
    Delete specified file
    åˆ é™¤æŒ‡å®šçš„æ–‡ä»¶
    """
    try:
        success = file_manager.delete_file(file_id)
        if success:
            return {"success": True, "message": t('file_delete_success')}
        else:
            raise HTTPException(status_code=404, detail=t('file_not_exist'))
    except Exception as e:
        raise HTTPException(status_code=500, detail=t('file_delete_failed', error=str(e)))

@app.post("/files/delete-bulk")
async def delete_files_bulk(request: BulkDeleteRequest):
    """
    Delete multiple files at once
    æ‰¹é‡åˆ é™¤å¤šä¸ªæ–‡ä»¶
    """
    try:
        success_count = 0
        failed_count = 0
        failed_files = []
        
        for file_id in request.file_ids:
            try:
                if file_manager.delete_file(file_id):
                    success_count += 1
                else:
                    failed_count += 1
                    failed_files.append(file_id)
            except Exception as e:
                failed_count += 1
                failed_files.append(file_id)
                main_logger.error(f"Failed to delete file {file_id}: {str(e)}")
        
        if success_count > 0 and failed_count == 0:
            return {
                "success": True, 
                "message": f"Successfully deleted {success_count} file(s)",
                "deleted_count": success_count
            }
        elif success_count > 0 and failed_count > 0:
            return {
                "success": True,
                "message": f"Deleted {success_count} file(s), failed to delete {failed_count} file(s)",
                "deleted_count": success_count,
                "failed_count": failed_count,
                "failed_files": failed_files
            }
        else:
            return {
                "success": False,
                "message": "Failed to delete all files",
                "failed_count": failed_count,
                "failed_files": failed_files
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Bulk delete operation failed: {str(e)}")

@app.post("/files/embed")
async def embed_files(request: FileEmbedRequest):
    """
    Batch embed documents into HyperRAG
    æ‰¹é‡åµŒå…¥æ–‡æ¡£åˆ°HyperRAG
    """
    if not HYPERRAG_AVAILABLE:
        raise HTTPException(status_code=500, detail="HyperRAG is not available")
    
    print(f"\n{'='*50}")
    print(t('document_embed_start', count=len(request.file_ids)))
    print(t('config_params', chunk_size=request.chunk_size, chunk_overlap=request.chunk_overlap))
    print(f"{'='*50}")
    
    results = []
    
    try:
        for i, file_id in enumerate(request.file_ids):
            try:
                print(f"\n{t('processing_file', current=i+1, total=len(request.file_ids), file_id=file_id)}")
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå¤„ç†ä¸­
                print(t('update_file_status_processing'))
                file_manager.update_file_status(file_id, "processing")
                
                # èŽ·å–æ–‡ä»¶ä¿¡æ¯
                print(t('get_file_info'))
                file_info = file_manager.get_file_by_id(file_id)
                if not file_info:
                    error_msg = t('file_not_found', path=file_id)
                    print(f"âŒ {error_msg}")
                    results.append({
                        "file_id": file_id,
                        "status": "error",
                        "error": t('file_not_exist')
                    })
                    continue
                
                print(f"âœ… {t('file_info', filename=file_info['filename'], size=file_info['file_size'])}")
                
                # ä½¿ç”¨æ–‡ä»¶å¯¹åº”çš„æ•°æ®åº“å
                database_name = file_info["database_name"]
                print(t('target_database', database=database_name))
                rag = get_or_create_hyperrag(database_name)
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                print(t('reading_file_content'))
                content = await file_manager.read_file_content(file_info["file_path"])
                print(f"âœ… {t('content_length', length=len(content))}")
                
                # æ’å…¥åˆ°HyperRAG
                print(t('start_document_embedding'))
                await rag.ainsert(content)
                print(f"âœ… {t('document_embedding_complete')}")
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå·²åµŒå…¥
                file_manager.update_file_status(file_id, "embedded")
                
                results.append({
                    "file_id": file_id,
                    "filename": file_info["filename"],
                    "database_name": database_name,
                    "status": "embedded"
                })
                
                print(f"âœ… {t('file_embedded_success', filename=file_info['filename'])}")
                
            except Exception as e:
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºé”™è¯¯
                error_msg = t('file_embedding_failed', file_id=file_id, error=str(e))
                print(f"âŒ {error_msg}")
                file_manager.update_file_status(file_id, "error", str(e))
                
                results.append({
                    "file_id": file_id,
                    "status": "error",
                    "error": str(e)
                })
        
        successful = len([r for r in results if r.get('status') == 'embedded'])
        print(f"\n{t('document_embedding_summary', success=successful, total=len(request.file_ids))}")
        print(f"{'='*50}")
        
        return {"embedded_files": results}
        
    except Exception as e:
        error_msg = t('batch_embedding_failed', error=str(e))
        print(f"âŒ {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)

# è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—é€šè¿‡WebSocketå‘é€
class WebSocketLogHandler(logging.Handler):
    def __init__(self, connection_manager):
        super().__init__()
        self.connection_manager = connection_manager
        
    def emit(self, record):
        try:
            log_message = self.format(record)
            # å¼‚æ­¥å‘é€æ—¥å¿—æ¶ˆæ¯
            asyncio.create_task(self.connection_manager.send_log_message({
                "type": "log",
                "level": record.levelname,
                "message": log_message,
                "timestamp": record.created,
                "logger_name": record.name
            }))
        except Exception:
            pass  # é¿å…æ—¥å¿—å¤„ç†å™¨è‡ªèº«é”™è¯¯å½±å“ä¸»ç¨‹åº

# è‡ªå®šä¹‰æµå¤„ç†å™¨ï¼Œæ•èŽ·printè¯­å¥å’Œå…¶ä»–è¾“å‡º
class WebSocketStreamHandler:
    def __init__(self, connection_manager, stream_type="stdout"):
        self.connection_manager = connection_manager
        self.stream_type = stream_type
        self.original_stream = sys.stdout if stream_type == "stdout" else sys.stderr
        
    def write(self, message):
        # åŒæ—¶å†™å…¥åŽŸå§‹æµ
        self.original_stream.write(message)
        self.original_stream.flush()
        
        # å‘é€åˆ°WebSocketï¼ˆåŽ»é™¤ç©ºè¡Œï¼‰
        if message.strip():
            asyncio.create_task(self.connection_manager.send_log_message({
                "type": "console",
                "level": "ERROR" if self.stream_type == "stderr" else "INFO",
                "message": message.strip(),
                "timestamp": asyncio.get_event_loop().time(),
                "source": self.stream_type
            }))
    
    def flush(self):
        self.original_stream.flush()

# WebSocketè¿žæŽ¥ç®¡ç†
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.logging_enabled = False
        self.original_stdout = None
        self.original_stderr = None

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        
        # å¦‚æžœæ˜¯ç¬¬ä¸€ä¸ªè¿žæŽ¥ï¼Œå¯ç”¨æ—¥å¿—é‡å®šå‘
        if len(self.active_connections) == 1 and not self.logging_enabled:
            self.enable_logging_redirect()

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        
        # å¦‚æžœæ²¡æœ‰è¿žæŽ¥äº†ï¼Œç¦ç”¨æ—¥å¿—é‡å®šå‘
        if len(self.active_connections) == 0 and self.logging_enabled:
            self.disable_logging_redirect()

    def enable_logging_redirect(self):
        """Enable log redirection / å¯ç”¨æ—¥å¿—é‡å®šå‘"""
        if not self.logging_enabled:
            self.original_stdout = sys.stdout
            self.original_stderr = sys.stderr
            
            # é‡å®šå‘æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
            sys.stdout = WebSocketStreamHandler(self, "stdout")
            sys.stderr = WebSocketStreamHandler(self, "stderr")
            
            self.logging_enabled = True
            print(t('log_redirect_enabled'))

    def disable_logging_redirect(self):
        """Disable log redirection / ç¦ç”¨æ—¥å¿—é‡å®šå‘"""
        if self.logging_enabled and self.original_stdout and self.original_stderr:
            sys.stdout = self.original_stdout
            sys.stderr = self.original_stderr
            self.logging_enabled = False
            print(t('log_redirect_disabled'))

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except Exception:
                # å¦‚æžœè¿žæŽ¥å·²æ–­å¼€ï¼Œæ ‡è®°ä¸ºç§»é™¤
                disconnected.append(connection)
        
        # ç§»é™¤æ–­å¼€çš„è¿žæŽ¥
        for conn in disconnected:
            self.disconnect(conn)

    async def send_progress_update(self, progress_data: dict):
        """Send progress updates to all connected clients / å‘é€è¿›åº¦æ›´æ–°åˆ°æ‰€æœ‰è¿žæŽ¥çš„å®¢æˆ·ç«¯"""
        message = json.dumps(progress_data)
        await self.broadcast(message)
    
    async def send_log_message(self, log_data: dict):
        """Send log messages to all connected clients / å‘é€æ—¥å¿—æ¶ˆæ¯åˆ°æ‰€æœ‰è¿žæŽ¥çš„å®¢æˆ·ç«¯"""
        message = json.dumps(log_data)
        await self.broadcast(message)

manager = ConnectionManager()

# è®¾ç½®å…¨é¢çš„æ—¥å¿—é…ç½®
def setup_comprehensive_logging():
    """Setup comprehensive logging configuration / è®¾ç½®å…¨é¢çš„æ—¥å¿—é…ç½®"""
    # è®¾ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    
    # æ¸…é™¤çŽ°æœ‰çš„å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # åˆ›å»ºWebSocketå¤„ç†å™¨
    ws_handler = WebSocketLogHandler(manager)
    ws_handler.setLevel(logging.INFO)
    # Use cleaner format: just time and message for INFO, full details for errors
    class CleanFormatter(logging.Formatter):
        def format(self, record):
            if record.levelno <= logging.INFO:
                # For INFO and below, use simple format
                return f"[{self.formatTime(record, '%H:%M:%S')}] {record.getMessage()}"
            else:
                # For WARNING and above, include more details
                return f"[{self.formatTime(record, '%H:%M:%S')}] {record.levelname} | {record.name} | {record.getMessage()}"
    
    formatter = CleanFormatter()
    ws_handler.setFormatter(formatter)
    
    # åˆ›å»ºæŽ§åˆ¶å°å¤„ç†å™¨ï¼ˆä¿ç•™æŽ§åˆ¶å°è¾“å‡ºï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger.addHandler(ws_handler)
    root_logger.addHandler(console_handler)
    
    # è®¾ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    logging.getLogger('hyperrag').setLevel(logging.INFO)
    logging.getLogger('openai').setLevel(logging.INFO)
    logging.getLogger('httpx').setLevel(logging.WARNING)  # å‡å°‘HTTPè¯·æ±‚æ—¥å¿—
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    
    # ç¡®ä¿HyperRAGç›¸å…³çš„æ‰€æœ‰å­æ¨¡å—éƒ½èƒ½è¾“å‡ºæ—¥å¿—
    hyperrag_modules = [
        'hyperrag.base',
        'hyperrag.hyperrag', 
        'hyperrag.llm',
        'hyperrag.operate',
        'hyperrag.prompt',
        'hyperrag.storage',
        'hyperrag.utils'
    ]
    
    for module_name in hyperrag_modules:
        module_logger = logging.getLogger(module_name)
        module_logger.setLevel(logging.INFO)
        # ç¡®ä¿æ¨¡å—æ—¥å¿—ä¹Ÿä¼šä¼ æ’­åˆ°æ ¹è®°å½•å™¨
        module_logger.propagate = True
    
    return root_logger

def configure_hyperrag_logging():
    """Configure HyperRAG-related detailed log output / é…ç½®HyperRAGç›¸å…³çš„è¯¦ç»†æ—¥å¿—è¾“å‡º"""
    try:
        # å¦‚æžœHyperRAGå¯ç”¨ï¼Œé…ç½®å…¶å†…éƒ¨æ—¥å¿—
        if HYPERRAG_AVAILABLE:
            # å¯¼å…¥HyperRAGç›¸å…³æ¨¡å—å¹¶è®¾ç½®æ—¥å¿—
            try:
                import hyperrag
                import hyperrag.base
                import hyperrag.storage
                import hyperrag.llm
                import hyperrag.utils
                
                # ä¸ºHyperRAGçš„ä¸»è¦æ¨¡å—è®¾ç½®æ—¥å¿—è®°å½•å™¨
                modules_to_configure = [
                    hyperrag,
                    hyperrag.base,
                    hyperrag.storage, 
                    hyperrag.llm,
                    hyperrag.utils
                ]
                
                for module in modules_to_configure:
                    if hasattr(module, '__name__'):
                        logger = logging.getLogger(module.__name__)
                        logger.setLevel(logging.INFO)
                        logger.propagate = True
                        
                print(f"âœ… {t('hyperrag_log_config_complete')}")
                        
            except ImportError as e:
                print(f"âš ï¸  {t('cannot_import_hyperrag_module', error=e)}")
                
    except Exception as e:
        print(f"âš ï¸  {t('hyperrag_log_config_failed', error=e)}")

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
main_logger = setup_comprehensive_logging()

# é…ç½®HyperRAGæ—¥å¿—
configure_hyperrag_logging()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # è¿™é‡Œå¯ä»¥å¤„ç†å®¢æˆ·ç«¯å‘é€çš„æ¶ˆæ¯
            await manager.send_personal_message(f"Message received: {data}", websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)

# å¸¦å®žæ—¶è¿›åº¦é€šçŸ¥çš„æ–‡æ¡£åµŒå…¥æŽ¥å£
@app.post("/files/embed-with-progress")
async def embed_files_with_progress(request: FileEmbedRequest):
    """
    Batch embed documents into HyperRAG with real-time progress notifications
    æ‰¹é‡åµŒå…¥æ–‡æ¡£åˆ°HyperRAGï¼Œå¸¦å®žæ—¶è¿›åº¦é€šçŸ¥
    """
    if not HYPERRAG_AVAILABLE:
        raise HTTPException(status_code=500, detail="HyperRAG is not available")
    
    # ç«‹å³è¿”å›žå¤„ç†å¼€å§‹çš„å“åº”
    total_files = len(request.file_ids)
    
    # å¼‚æ­¥å¤„ç†æ–‡ä»¶åµŒå…¥
    asyncio.create_task(process_files_with_progress(request, total_files))
    
    return {
        "message": t('document_embedding_started'),
        "total_files": total_files,
        "processing": True
    }

async def process_files_with_progress(request: FileEmbedRequest, total_files: int):
    """Asynchronously process file embedding and send progress updates / å¼‚æ­¥å¤„ç†æ–‡ä»¶åµŒå…¥å¹¶å‘é€è¿›åº¦æ›´æ–°"""
    try:
        print(f"="*60)
        print(t('batch_file_embed_start'))
        print(t('total_files', count=total_files))
        print(t('config_params', chunk_size=request.chunk_size, chunk_overlap=request.chunk_overlap))
        print(f"="*60)
        
        main_logger.info(t('processing_embed_task', count=total_files))
        main_logger.info(t('config_parameters', chunk_size=request.chunk_size, chunk_overlap=request.chunk_overlap))
        
        successful_files = 0
        failed_files = 0
        
        for i, file_id in enumerate(request.file_ids):
            try:
                print(f"\n{'='*40}")
                print(t('processing_file_num', current=i + 1, total=total_files))
                print(t('file_id_label', file_id=file_id))
                print(f"{'='*40}")
                
                # å‘é€è¿›åº¦æ›´æ–°
                await manager.send_progress_update({
                    "type": "progress",
                    "file_id": file_id,
                    "current": i + 1,
                    "total": total_files,
                    "percentage": ((i + 1) / total_files) * 100,
                    "status": "processing",
                    "message": t('processing_file_message', current=i + 1, total=total_files)
                })
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå¤„ç†ä¸­
                print(t('update_file_status_processing'))
                file_manager.update_file_status(file_id, "processing")
                
                # èŽ·å–æ–‡ä»¶ä¿¡æ¯
                print(t('reading_file_content_progress'))
                main_logger.info(t('getting_file_info', file_id=file_id))
                file_info = file_manager.get_file_by_id(file_id)
                if not file_info:
                    error_msg = t('file_not_found', path=file_id)
                    print(f"âŒ {t('error_label', error=error_msg)}")
                    main_logger.error(error_msg)
                    await manager.send_progress_update({
                        "type": "error",
                        "file_id": file_id,
                        "error": t('file_not_exist'),
                        "current": i + 1,
                        "total": total_files
                    })
                    failed_files += 1
                    continue
                
                print(f"âœ… {t('file_info_success')}")
                print(f"  - {t('filename', filename=file_info['filename'])}")
                print(f"  - {t('file_size', size=file_info['file_size'])}")
                print(f"  - {t('upload_time', time=file_info['upload_time'])}")
                
                # ä½¿ç”¨è¯·æ±‚ä¸­æŒ‡å®šçš„æ•°æ®åº“ï¼Œå¦‚æžœæ²¡æœ‰åˆ™ä½¿ç”¨æ–‡ä»¶å¯¹åº”çš„æ•°æ®åº“å
                database_name = request.database if request.database else file_info.get("database_name", "default")
                print(f"  - {t('target_database', database=database_name)}")
                
                main_logger.info(t('starting_file_processing', filename=file_info['filename'], size=file_info['file_size'], database=database_name))
                
                # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆå§‹åŒ–å¯¹åº”çš„HyperRAGå®žä¾‹
                print(t('initializing_hyperrag_instance'))
                main_logger.info(t('initializing_hyperrag_with_db', database=database_name))
                rag = get_or_create_hyperrag(database_name)
                print(f"âœ… {t('hyperrag_instance_initialized')}")
                main_logger.info(t('hyperrag_initialized_with_db', database=database_name))
                
                # å‘é€è¯¦ç»†è¿›åº¦ä¿¡æ¯
                await manager.send_progress_update({
                    "type": "file_processing",
                    "file_id": file_id,
                    "filename": file_info["filename"],
                    "database_name": database_name,
                    "stage": "reading",
                    "message": t('reading_file_message', filename=file_info['filename'], database=database_name)
                })
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                print(t('reading_file_content'))
                main_logger.info(t('reading_file_message', filename=file_info['filename'], database=database_name))
                content = await file_manager.read_file_content(file_info["file_path"])
                print(f"âœ… {t('file_read_complete', length=len(content))}")
                main_logger.info(t('file_read_complete', length=len(content)))
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                preview = content[:200] + "..." if len(content) > 200 else content
                print(t('content_preview', preview=preview))
                
                # å‘é€åµŒå…¥é˜¶æ®µçš„è¿›åº¦
                await manager.send_progress_update({
                    "type": "file_processing",
                    "file_id": file_id,
                    "filename": file_info["filename"],
                    "database_name": database_name,
                    "stage": "embedding",
                    "message": t('embedding_document_message', filename=file_info['filename'], database=database_name)
                })
                
                # æ’å…¥åˆ°HyperRAG
                print(t('document_embedding_processing'))
                print(t('document_embedding_wait'))
                main_logger.info(t('embedding_document_message', filename=file_info['filename'], database=database_name))
                main_logger.info(t('document_chunking'))
                
                # è¿™é‡Œä¼šè§¦å‘HyperRAGçš„è¯¦ç»†å¤„ç†è¿‡ç¨‹
                await rag.ainsert(content)
                
                print(f"âœ… {t('document_embedding_complete')}")
                main_logger.info(t('file_embed_complete', filename=file_info['filename'], database=database_name))
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå·²åµŒå…¥
                file_manager.update_file_status(file_id, "embedded")
                
                # å‘é€æˆåŠŸå®Œæˆçš„è¿›åº¦æ›´æ–°
                await manager.send_progress_update({
                    "type": "file_completed",
                    "file_id": file_id,
                    "filename": file_info["filename"],
                    "database_name": database_name,
                    "status": "completed",
                    "message": t('file_embed_complete', filename=file_info['filename'], database=database_name)
                })
                
                successful_files += 1
                print(f"âœ… {t('file_process_success', filename=file_info['filename'])}")
                
            except Exception as e:
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºé”™è¯¯
                error_msg = t('file_process_failed', file_id=file_id, error=str(e))
                print(f"âŒ {error_msg}")
                main_logger.error(error_msg)
                file_manager.update_file_status(file_id, "error", str(e))
                
                # å‘é€é”™è¯¯è¿›åº¦æ›´æ–°
                await manager.send_progress_update({
                    "type": "file_error",
                    "file_id": file_id,
                    "error": str(e),
                    "current": i + 1,
                    "total": total_files
                })
                
                failed_files += 1
        
        # å‘é€æ•´ä½“å®Œæˆçš„è¿›åº¦æ›´æ–°
        print(f"\n{'='*60}")
        print(t('batch_document_complete'))
        print(t('total_files', count=total_files))
        print(t('success_process', count=successful_files))
        print(t('failed_process', count=failed_files))
        print(t('success_rate', rate=f"{(successful_files/total_files)*100:.1f}"))
        print(f"{'='*60}")
        
        main_logger.info(t('all_documents_complete', total=total_files, success=successful_files, failed=failed_files))
        await manager.send_progress_update({
            "type": "all_completed",
            "message": t('all_complete_message', success=successful_files, failed=failed_files),
            "total_files": total_files,
            "successful_files": successful_files,
            "failed_files": failed_files
        })
        
    except Exception as e:
        # å‘é€æ•´ä½“é”™è¯¯ä¿¡æ¯
        error_msg = t('batch_embedding_failed', error=str(e))
        print(f"âŒ {error_msg}")
        main_logger.error(error_msg)
        await manager.send_progress_update({
            "type": "error",
            "error": error_msg
        })